# RAG System - Система вопросов и ответов на основе документов

Интеллектуальная система для работы с PDF документами с использованием технологий RAG (Retrieval-Augmented Generation). Система обеспечивает загрузку, индексацию и интеллектуальный поиск по документам с возможностью получения ответов на вопросы на основе загруженного контента.

![Ollama](https://img.shields.io/badge/ollama-0.1.8-blue)
![Python](https://img.shields.io/badge/python-3.8+-green)
![Streamlit](https://img.shields.io/badge/streamlit-1.32.0-red)

## Основные возможности

- **Обработка PDF документов** - Загрузка и извлечение текста из больших PDF файлов
- **Векторный поиск** - Быстрый и точный поиск по содержимому документов
- **Умная маршрутизация** - Автоматическое определение релевантности запросов
- **Поддержка языков** - Русский и казахский языки
- **Локальная обработка** - Работа без интернета через Ollama
- **Интерактивный интерфейс** - Удобный веб-интерфейс на Streamlit
- **Отладочная информация** - Детальная информация о работе системы в режиме отладки

## Технологический стек

- **LLM**: Ollama (поддержка различных моделей: llama3.2, deepseek-r1)
- **Embeddings**: Ollama embeddings (nomic-embed-text, mxbai-embed-large)
- **Vector Database**: ChromaDB
- **Text Processing**: LangChain, PyMuPDF
- **Web Interface**: Streamlit
- **Document Processing**: PyMuPDF для извлечения текста

## Требования

### Системные требования
- Python 3.8+
- 4+ GB RAM (рекомендуется 8+ GB)
- 10+ GB свободного места на диске

### Зависимости
Все зависимости указаны в `requirements.txt`:
- streamlit==1.32.0
- langchain==0.1.16
- langchain-community==0.0.34
- chromadb==0.4.24
- ollama==0.1.8
- PyMuPDF==1.24.1
- sentence-transformers==2.6.1
- python-dotenv==1.0.1
- numpy==1.26.4
- pandas==2.2.1
- tiktoken==0.6.0
- requests==2.31.0

## Установка и настройка

### 1. Клонирование и подготовка окружения

```bash
git clone <repository-url>
cd rag-project

# Создание виртуального окружения
python -m venv venv

# Активация окружения
# Windows:
venv\\Scripts\\activate
# Linux/Mac:
source venv/bin/activate

# Установка зависимостей
pip install -r requirements.txt
```

### 2. Установка и настройка Ollama

```bash
# Установка Ollama (следуйте инструкциям на https://ollama.ai/)

# Запуск сервиса Ollama
ollama serve

# Установка необходимых моделей
ollama pull llama3.2:latest
ollama pull nomic-embed-text:latest

# Дополнительные рекомендуемые модели
ollama pull deepseek-r1:latest
ollama pull mxbai-embed-large:latest
```

### 3. Запуск системы

```bash
streamlit run app.py
```

Система будет доступна по адресу: `http://localhost:8501`

## Использование

### 1. Инициализация системы
При первом запуске система автоматически:
- Проверит доступность Ollama
- Протестирует LLM и embedding модели
- Инициализирует векторную базу данных

### 2. Загрузка документов
- Перейдите в раздел **"Управление документами"**
- Выберите способ загрузки:
  - Загрузка файлов через интерфейс
  - Загрузка из папки
  - Переиндексация существующих документов

### 3. Работа с чатом
- Перейдите в раздел **"Чат"**
- Задавайте вопросы по загруженным документам
- Система предоставит ответы со ссылками на источники

### 4. Настройки
В разделе **"Настройки"** доступны:
- Выбор LLM и embedding моделей
- Настройка параметров маршрутизации
- Экспорт истории разговоров
- Системная информация
- Режим отладки

## Архитектура системы

### Основные компоненты

```
src/
├── main.py              # Основной RAG пайплайн
├── document_processor.py # Обработка PDF документов
├── vector_store.py       # Работа с векторной БД
├── llm_manager.py       # Управление LLM моделями  
├── router.py            # Умная маршрутизация запросов
└── config.py            # Управление конфигурацией
```

### Поток данных

1. **Загрузка документов** → Извлечение текста → Разбиение на фрагменты
2. **Генерация эмбеддингов** → Сохранение в ChromaDB
3. **Обработка запроса** → Поиск похожих фрагментов → Маршрутизация
4. **Генерация ответа** → Формирование ответа с источниками

## Структура проекта

```
rag-project/
├── app.py                 # Главное Streamlit приложение
├── requirements.txt       # Зависимости Python
├── config.json           # Конфигурация моделей
├── src/                  # Исходный код
│   ├── __init__.py
│   ├── main.py           # RAG пайплайн
│   ├── document_processor.py
│   ├── vector_store.py
│   ├── llm_manager.py
│   ├── router.py
│   └── config.py
├── data/                 # Данные системы
│   ├── documents/        # PDF файлы
│   └── chroma_db/        # Векторная база данных
└── venv/                 # Виртуальное окружение
```

## Конфигурация

### Файл config.json
```json
{
  "llm_model": "llama3.2:latest",
  "embedding_model": "nomic-embed-text:latest"
}
```

### Настройки системы
- **Размер фрагмента**: 1000 символов
- **Перекрытие фрагментов**: 200 символов  
- **Количество результатов поиска**: 20
- **Порог уверенности**: 0.5

## Функциональные особенности

### Обработка документов
- Извлечение текста из PDF с сохранением структуры
- Интеллектуальная очистка текста
- Разбиение на фрагменты с контекстом
- Сохранение метаданных (страницы, разделы)

### Поиск и ранжирование
- Векторный поиск по косинусному сходству
- Бустинг релевантности по ключевым словам
- Учет структурной информации документа
- Фильтрация по выбранным документам

### Умная маршрутизация
- Анализ типа и языка запроса
- Определение релевантности найденного контекста
- Автоматический выбор стратегии ответа
- Поддержка fallback-режимов

## Мониторинг и логирование

Система выводит логи в консоль:
- Обработанные запросы
- Время отклика
- Качество поиска
- Ошибки и предупреждения

## Устранение неполадок

### Проблемы с Ollama
```bash
# Проверка статуса сервиса
curl http://localhost:11434/api/version

# Перезапуск сервиса
ollama serve
```

### Проблемы с моделями
```bash
# Проверка установленных моделей
ollama list

# Переустановка модели
ollama pull llama3.2:latest
```

### Проблемы с векторной БД
- Очистка базы через интерфейс: Управление документами → Очистить всю векторную базу
- Или удаление папки: `rm -rf data/chroma_db`

## Производительность

### Рекомендации по оптимизации
- Используйте SSD для хранения векторной БД
- Выделите достаточно RAM для больших документов
- Оптимизируйте размер фрагментов для вашего типа документов

### Масштабирование
- Система поддерживает тысячи документов
- Время индексации зависит от размера файлов
- Поиск работает в режиме реального времени

## Разработка

### Добавление новых функций
1. Создайте новый модуль в `src/`
2. Интегрируйте с основным пайплайном в `main.py`
3. Добавьте интерфейс в `app.py`

### Тестирование
```bash
# Тестирование отдельных компонентов
python -m src.document_processor
python -m src.vector_store
```

## Лицензия

Этот проект разработан для внутреннего использования. 

## Поддержка

При возникновении проблем:
1. Проверьте логи в консоли
2. Убедитесь, что Ollama запущен и модели установлены
3. Проверьте системные требования